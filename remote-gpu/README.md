# CuteCaption Remote GPU System

**Complete implementation for offloading GPU-intensive AI inference to RunPod servers.**

---

## üéØ Overview

The Remote GPU System allows CuteCaption users to offload **VLM (Qwen3-VL)**, **Semantic (CLIP-SAE)**, and **Pose (MediaPipe)** processing to powerful remote GPUs (RTX 4090, RTX 5090, etc.) running on RunPod.

### Key Benefits:
- ‚úÖ **No local RAM/VRAM limits** - Run Qwen3-VL on any machine
- ‚úÖ **Faster inference** - Leverage 4090/5090 GPUs
- ‚úÖ **Seamless experience** - User-controlled, feels local
- ‚úÖ **Cost-effective** - Pay only when testing ($0.60-1.50/hour)
- ‚úÖ **Zero config** - One-click RunPod deployment

---

## üöÄ Quick Start for Users

### Step 1: Launch on RunPod

1. **Find the Template**:
   - Go to [RunPod Templates](https://www.runpod.io/console/gpu-cloud) (or use the direct link provided by CuteCaption).
   - Search for **"CuteCaption Remote GPU"**.

2. **Configure & Deploy**:
   - Click **Deploy**.
   - **GPU Selection**: Choose **RTX 4090** (Recommended) or **RTX 5090** (Fastest).
   - **Environment Variables**:
     - `CUTECAPTION_API_KEY`: Set this to a secure password/key (you will need this later).
   - Click **Continue** -> **Deploy**.

3. **Get Connection Details**:
   - Go to **My Pods**.
   - Wait for the pod to show **Running**.
   - Click **Connect**.
   - Copy the **HTTP Service** URL (e.g., `https://abc123-8080.proxy.runpod.net`).
     *Note: Use the port 8080 link, not the SSH one.*

### Step 2: Configure CuteCaption

1. Open **CuteCaption**.
2. Go to **Settings ‚Üí AI Intelligence**.
3. Scroll to **Remote GPU (RunPod)**.
4. Toggle **Enable Remote GPU**.
5. Enter **Server URL**: Paste the URL from Step 1 (e.g., `https://abc123-8080.proxy.runpod.net`).
6. Enter **API Key**: The value you set for `CUTECAPTION_API_KEY`.
7. Click **Test Connection**.
8. ‚úÖ Success! Save settings.

### Step 3: Verify

1. Load a dataset in CuteCaption.
2. Click **Analyze Dataset**.
3. Open the **RunPod Dashboard** (navigate to your Server URL in a web browser) to see real-time metrics and logs.

---

## üì¶ Components

### 1. **Remote GPU Server** (`remote-gpu/server/`)
- **FastAPI service** running on RunPod
- Lazy-loads models (VLM/CLIP/Pose) on first request
- WebSocket for real-time progress updates
- Beautiful status dashboard at `:8080`

### 2. **Status Dashboard** (`remote-gpu/dashboard/`)
- Real-time GPU/RAM/VRAM monitoring
- Service status (VLM, Semantic, Pose)
- Connection info (URL, API key)
- Activity logs

### 3. **CuteCaption Client** (`modules/main-process/remote-gpu-client.js`)
- HTTP REST client for inference requests
- WebSocket listener for live updates
- Request caching (5 min TTL)
- Auto-reconnect on disconnect

### 4. **BaseService Integration** (`modules/main-process/intelligence-services/base-service.js`)
- When Remote GPU enabled ‚Üí routes to remote server
- When disabled ‚Üí runs locally (original behavior)
- User has complete control (no automatic fallback)

### 5. **Settings UI** (`renderer/modules/ai-intelligence-settings.js`)
- **Toggle**: Enable/Disable Remote GPU
- **Server URL**: e.g., `http://your-runpod-url:8080`
- **API Key**: From RunPod env variable
- **Test Connection**: Verify before saving

---

## üöÄ Quick Start

### Step 1: Deploy to RunPod

1. **Build Docker image** (from `working_8_30` directory):
   ```bash
   docker build -t your-username/cutecaption-remote-gpu:latest -f remote-gpu/Dockerfile .
   docker push your-username/cutecaption-remote-gpu:latest
   ```

2. **Create RunPod Template**:
   - Go to https://www.runpod.io/console/user/templates
   - Click "New Template"
   - Container Image: `your-username/cutecaption-remote-gpu:latest`
   - Container Disk: `20 GB`
   - Volume: `50 GB` ‚Üí `/workspace/models`
   - Expose Ports: `8080`
   - Environment Variables:
     ```
     CUTECAPTION_API_KEY=your-secure-key-here
     ```

3. **Deploy Pod**:
   - Select GPU: **RTX 4090** or **RTX 5090**
   - Start Pod
   - Copy Pod URL (e.g., `https://abc123-8080.proxy.runpod.net`)

### Step 2: Configure CuteCaption

1. Open **Settings ‚Üí AI Intelligence**
2. Scroll to **Remote GPU (RunPod)**
3. Enable **Use Remote GPU**
4. Enter **Server URL**: `https://abc123-8080.proxy.runpod.net`
5. Enter **API Key**: `your-secure-key-here`
6. Click **Test Connection**
7. ‚úÖ Success! Save settings

### Step 3: Test

1. Load a dataset in CuteCaption
2. Click **Analyze Dataset** ‚Üí Remote GPU handles VLM classification
3. Ask a question ‚Üí Remote GPU handles VLM synthesis
4. Monitor performance in RunPod dashboard (open Server URL in browser)

---

## üîß Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CuteCaption (Electron)                   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  IntelligenceOrchestrator                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ BaseService  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ RemoteGPUClient          ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (VLM/Sem/   ‚îÇ    ‚îÇ - HTTP REST              ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Pose)       ‚îÇ    ‚îÇ - WebSocket              ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ - Caching                ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                      ‚îÇ - Auto-reconnect         ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ HTTPS
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  RunPod Pod (RTX 4090/5090)                  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  FastAPI Server (main.py)                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ VLMService   ‚îÇ  ‚îÇ SemanticSvc  ‚îÇ  ‚îÇ PoseSvc     ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (Qwen3-VL)   ‚îÇ  ‚îÇ (CLIP-SAE)   ‚îÇ  ‚îÇ (MediaPipe) ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Status Dashboard (index.html)                    ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Real-time GPU/RAM metrics                     ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Service status                                ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Activity logs                                 ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Request Flow:

1. **User clicks "Analyze"** in CuteCaption
2. **Orchestrator** ‚Üí checks if Remote GPU enabled
3. **If enabled** ‚Üí `BaseService._requestViaRemoteGPU()`
4. **RemoteGPUClient** ‚Üí HTTP POST to RunPod server
5. **FastAPI server** ‚Üí lazy-loads model (if not cached)
6. **Python service** ‚Üí runs inference
7. **Response** ‚Üí cached and returned to CuteCaption
8. **UI updates** ‚Üí seamless, user sees results

---

## ‚öôÔ∏è Configuration

### Environment Variables (RunPod)

| Variable | Default | Description |
|----------|---------|-------------|
| `CUTECAPTION_API_KEY` | `dev-key` | **Required** - API key for authentication |
| `CUTECAPTION_HOST` | `0.0.0.0` | Server bind address |
| `CUTECAPTION_PORT` | `8080` | Server port |
| `CUTECAPTION_MODEL_CACHE` | `/workspace/models` | HuggingFace model cache directory |

### CuteCaption Settings (electron-store)

| Key | Default | Description |
|-----|---------|-------------|
| `ai_intelligence_remote_gpu_enabled` | `0` | Enable/disable remote GPU |
| `ai_intelligence_remote_gpu_url` | `''` | RunPod server URL |
| `ai_intelligence_remote_gpu_api_key` | `''` | API key |

---

## üîí Security

### Best Practices:

1. **Strong API Key**: Use a long, random string (32+ chars)
2. **HTTPS**: RunPod provides HTTPS proxies automatically
3. **IP Whitelisting**: In RunPod dashboard, restrict access to your IP
4. **Secrets**: Use RunPod Secrets for API keys (not env vars)
5. **Auto-Stop**: Enable auto-stop when idle to prevent unauthorized use

### API Key Management:

```bash
# Generate secure API key
openssl rand -base64 32

# Set in RunPod template
CUTECAPTION_API_KEY=your-generated-key-here
```

---

## üìä Performance

### Latency Comparison (VLM Caption):

| Setup | First Request | Subsequent | Notes |
|-------|---------------|------------|-------|
| **Local (Low RAM)** | 60-120s | 30-60s | Model loading thrashes RAM |
| **Local (High RAM)** | 30s | 2-5s | Model stays in VRAM |
| **Remote (4090)** | 35s | 2-4s | +200ms network latency |
| **Remote (5090)** | 25s | 1-3s | Faster inference |

### Cost Analysis:

| GPU | Hourly Cost | 100 Captions | 1000 Captions |
|-----|-------------|--------------|---------------|
| RTX 4090 | $0.79/hr | ~$0.10 | ~$1.00 |
| RTX 5090 | $1.49/hr | ~$0.15 | ~$1.50 |

**Testing tip**: Enable auto-stop after 5 min idle to minimize costs.

---

## üêõ Troubleshooting

### "Connection failed"
- ‚úÖ Check RunPod Pod is running
- ‚úÖ Verify Server URL (include `http://` or `https://`)
- ‚úÖ Ensure port `8080` is exposed
- ‚úÖ Test: `curl https://your-pod-url:8080/status`

### "Invalid API key"
- ‚úÖ Verify API key matches in RunPod env var
- ‚úÖ Check for extra spaces/newlines
- ‚úÖ Recreate Pod with correct env var

### "Models downloading slowly"
- ‚úÖ Use persistent volume for `/workspace/models`
- ‚úÖ Pre-bake models into Docker image (uncomment in Dockerfile)
- ‚úÖ Choose RunPod datacenter with good bandwidth

### "Request timeout"
- ‚úÖ First request takes 30-60s (model loading)
- ‚úÖ Check GPU has enough VRAM (24GB+ for VLM)
- ‚úÖ Monitor RunPod dashboard logs

---

## üé® Dashboard Features

Access dashboard at `http://your-pod-url:8080`:

- **GPU Metrics**: Real-time VRAM usage, utilization, temperature
- **System Metrics**: CPU, RAM, disk usage
- **Service Status**: VLM/Semantic/Pose loaded state
- **Connection Info**: Copy-paste server URL, API endpoint
- **Activity Logs**: See all inference requests
- **Controls**: Start/stop services (preload models)

---

## üìù API Endpoints

### VLM

- `POST /api/v1/vlm/classify` - Image classification
- `POST /api/v1/vlm/caption` - Image captioning
- `POST /api/v1/vlm/verify` - Candidate verification
- `POST /api/v1/vlm/synthesize` - Answer synthesis

### Semantic

- `POST /api/v1/semantic/search` - CLIP search
- `POST /api/v1/semantic/embed` - Generate embeddings

### Pose

- `POST /api/v1/pose/detect` - MediaPipe pose detection

### System

- `GET /status` - Server health (no auth required)
- `GET /dashboard` - Status dashboard (no auth required)
- `WS /ws` - WebSocket for live updates

---

## üîÑ Updates & Maintenance

### Update Remote GPU Server:

```bash
# Rebuild image with latest code
docker build -t your-username/cutecaption-remote-gpu:latest -f remote-gpu/Dockerfile .
docker push your-username/cutecaption-remote-gpu:latest

# In RunPod: Stop Pod ‚Üí Restart Pod (pulls latest image)
```

### Monitor Logs:

- RunPod Console ‚Üí Pods ‚Üí Your Pod ‚Üí Logs
- Look for `[VLMService]`, `[SemanticService]` messages

---

## üí° Tips & Tricks

### 1. **Pre-cache Models**
Uncomment in `Dockerfile` to bake models into image:
```dockerfile
RUN python3 -c "from transformers import AutoModel; \
    AutoModel.from_pretrained('Qwen/Qwen2-VL-7B-Instruct', \
    cache_dir='/workspace/models')"
```
**Trade-off**: 30GB+ image, but instant startup.

### 2. **Multiple Users**
Share one RunPod Pod across team:
- Everyone uses same Server URL + API Key
- Models load once, serve all users
- Cost-effective for teams

### 3. **Development Workflow**
- Keep RunPod Pod running during testing
- Enable in Settings only when needed
- Disable when running local tests

### 4. **Batch Processing**
Remote GPU Client automatically batches concurrent requests for efficiency.

---

## ü§ù Support

- **Discord**: [Join our Discord](https://discord.gg/2gQ5eBtRCj)
- **GitHub**: [CuteCaption Repository](https://github.com/cutecaption)
- **Issues**: [Report Issues](https://github.com/cutecaption/issues)

---

## üìú License

Same as CuteCaption.

---

## üõ†Ô∏è Advanced: Building the Template (For Developers)

If you want to build your own version of the Docker image or contribute to the template:

1. **Build Docker image** (from `working_8_30` directory):
   ```bash
   docker build -t your-username/cutecaption-remote-gpu:latest -f remote-gpu/Dockerfile .
   docker push your-username/cutecaption-remote-gpu:latest
   ```

2. **Create RunPod Template**:
   - Go to Templates -> New Template.
   - Image: `your-username/cutecaption-remote-gpu:latest`
   - Ports: `8080` (HTTP)
   - Volume: `/workspace/models` (50GB)
   - Env Vars: `CUTECAPTION_API_KEY` (Required)

---

## üéâ Credits

Built with:
- [FastAPI](https://fastapi.tiangolo.com/) - High-performance web framework
- [RunPod](https://www.runpod.io/) - GPU cloud infrastructure
- [Qwen3-VL](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) - Vision-language model
- [CLIP-SAE](https://huggingface.co/zer0int/LongCLIP-SAE-ViT-L-14) - Semantic search
- [MediaPipe](https://google.github.io/mediapipe/) - Pose detection

---

**Made with ‚ù§Ô∏è for seamless AI inference offloading**

